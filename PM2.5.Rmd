---
title: "PM2.5"
output:
  html_document: default
  pdf_document: default
  word_document: default
Binome: LIU HAO, SAN Scochivin
---

## Introduction
\textbf{PM2.5} refers to atmospheric particulate matter (PM) that have a diameter of less than 2.5 micrometers, which is about $3\%$ the diameter of a human hair.
\\
In this project, our objective is to predict \textbf{PM2.5} in the future by building a linear regression model.
Problems:
1: Certain PM2.5 values (Y) are missing NA
2: Quite a lot instances, low performance with a normal computer
3: Four variables they are in some way related with each other. (Year, Month, Day, Hour)
4: Categorical variable instead of numerical variable ( Combined wind direction ) 
## Data description and Descriptive analysis
This data set has been collected in Beijing and it consists 13 varaibles and 43824 observations.
*Beijing PM2.5 Data Set* 
*Data Set Information:*
*The date time period is between Jan 1st, 2010 to Dec 31st, 2014*
\begin{itemize}
\item No: row number \\
\item Year: year of data in this row\\ 
\item Month: month of data in this row\\ 
\item Day: day of data in this row \\
\item Hour: hour of data in this row \\
\item PM2.5: PM2.5 concentration $(ug/m^3)$\\ 
\item DEWP: Dew Point \\
\item TEMP: Temperature\\
\item PRES: Pressure (hPa)\\
\item cbwd: Combined wind direction\\ 
\item Iws: Cumulated wind speed (m/s)\\ 
\item Is: Cumulated hours of snow \\
\item Ir: Cumulated hours of rain \\
\end{itemize}

####Beijing PM2.5 Data Set 
####Data Set Information:
####The date time period is between Jan 1st, 2010 to Dec 31st, 2014
```{r data processing unit, echo=FALSE}
#Initialize environment
rm(list=ls())
graphics.off()

#Read data set
dataset = read.csv("dataset.csv",sep = ",",header = TRUE)
#Check orignal dimension
dim(dataset)
#Remove index column 
dataset = dataset[,2:ncol(dataset)]
Y = dataset[,'pm2.5']
Y = Y[1:length(Y)-1]
dataset = dataset[2:nrow(dataset),]
#Add Yn-1 as varaible to Yn time serie
dataset = cbind(dataset,Y)

day_of_year <- function(year,month,day)
{
  if(year%%4!=0){
    if(month==1)
      dayofyear = day
    if(month==2)
      dayofyear = 31+day
    if(month==3)
      dayofyear = 31+28+day
    if(month==4)
      dayofyear = 31+28+31+day
    if(month==5)
      dayofyear = 31+28+31+30+day
    if(month==6)
      dayofyear = 31+28+31+30+31+day
    if(month==7)
      dayofyear = 31+28+31+30+31+30+day
    if(month==8)
      dayofyear = 31+28+31+30+31+30+31+day
    if(month==9)
      dayofyear = 31+28+31+30+31+30+31+31+day
    if(month==10)
      dayofyear = 31+28+31+30+31+30+31+31+30+day
    if(month==11)
      dayofyear = 31+28+31+30+31+30+31+31+30+31+day
    if(month==12)
      dayofyear = 31+28+31+30+31+30+31+31+30+31+30+day
  }
  if(year%%4==0){
    if(month==1)
      dayofyear = day
    if(month==2)
      dayofyear = 31+day
    if(month==3)
      dayofyear = 31+29+day
    if(month==4)
      dayofyear = 31+29+31+day
    if(month==5)
      dayofyear = 31+29+31+30+day
    if(month==6)
      dayofyear = 31+29+31+30+31+day
    if(month==7)
      dayofyear = 31+29+31+30+31+30+day
    if(month==8)
      dayofyear = 31+29+31+30+31+30+31+day
    if(month==9)
      dayofyear = 31+29+31+30+31+30+31+31+day
    if(month==10)
      dayofyear = 31+29+31+30+31+30+31+31+30+day
    if(month==11)
      dayofyear = 31+29+31+30+31+30+31+31+30+31+day
    if(month==12)
      dayofyear = 31+29+31+30+31+30+31+31+30+31+30+day
  }
  return(dayofyear)
}

day = 0
for (i in c(1:nrow(dataset)))
{
  day[i] = day_of_year(dataset[i,1],dataset[i,2],dataset[i,3])
}
dataset[,'day'] = day 
day  = dataset[,'day']
month = dataset[,'month']
hour = dataset[,'hour'] 
sin_month = sin(month*2*pi/12)
cos_month = cos(month*2*pi/12)
sin_day = 0
cos_day = 0
for (i in c(1:nrow(dataset)))
{
  if(dataset[i,'year']%%4!=0)
  {
    sin_day[i] = sin(day[i]*2*pi/365)
    cos_day[i] = cos(day[i]*2*pi/365)
  }
  if(dataset[i,'year']%%4==0)
  {
    sin_day[i] = sin(day[i]*2*pi/366)
    cos_day[i] = cos(day[i]*2*pi/366)
  }
}
sin_hour = sin(hour*2*pi/24)
cos_hour = cos(hour*2*pi/24)

#Delete useless varaibles
dataset[,'month'] <- NULL
dataset[,'day'] <- NULL
dataset[,'hour'] <- NULL

#Combine all together
dataset = cbind(dataset,sin_month,cos_month,sin_day,cos_day,sin_hour,cos_hour)

#Remove NA data from dataset
dataset = na.omit(dataset)
#Check demision again
dim(dataset)
#Check dataset
head(dataset)
#dummy codage
library(dummies)
cbwd = dummy(dataset[,'cbwd'])
dataset = cbind(dataset,cbwd)
#remove cbwd column
dataset[,'cbwd'] <- NULL
#dim(dataset)
#summary(dataset)
```
## Descriptive analysis
### Response variable : **pm2.5**
In this data set, the response variable \textbf{pm2.5} takes value between $0$ and $ 994$ with average value of $98.6$, variance of $8470.619$ and other statistical descriptions as shown in the below table.
```{r, echo=FALSE}
summary(dataset$pm2.5)
```

This variable appears not to follow the Guassian distribution by immediately look at its distribution in the below graphic.

Moreover, depending on the Boxplot, we can see that there are some weired data points which will be considered as another problem for our regression. However, we will try to see the effect of those outlier in our model later.

```{r, echo=FALSE}
par(mfrow= c(1,3))
hist(dataset$pm2.5, freq = FALSE, col = "cyan", main = "Histogram", xlab = "")
lines(density(dataset$pm2.5), col="darkred")
boxplot(dataset$pm2.5, main="Boxplot", col = "red")
plot(ecdf(dataset$pm2.5), main="Repartition Function", xlab="", col = "red")
title(outer = TRUE, main = "\n Distribution of pm2.5")

```

### Predictors and correlation
Firstly there are 12 predictors in our data set and one of them are categorical variable.After that, we transformed time variables into two dimensions and the categorical variable into binary. And we also add variable pm2.5 at time $t_(n-1)$. So that we have 18 preditors.
In this part, we will focus on two important kind of correlations. One is between independent variables, and another one is between independent and response variable . Since there are two kinds of predictors in our data set, we will look at those relationships separately.

The figure below contains informations of correlations between numerical variables. We can see that there are some pairs of predictors appear to be highly correlated with each other which is what we don't want to see. These \textbf{redundant} variables will lead to overfitting in our regression. On the other hand, we also can see that there is no predictor appear to be highly correlated with the response variable(except variable *Y*(pm2.5 at $t_(n-1)$) which is not so good for our regression as well. The high correlations between *Y* and response variable will lead to good prediction (large value of $r^2$ ). That's why we needed to add the variable *Y*.

```{r pressure, echo=FALSE}
#take 8000 sample randomly from orginal dataset and getnerate a subset
n = 40000
sample = sample(c(1:nrow(dataset)),n)
subset = dataset[sample,]

#PLot to see variables' distrubution
# Density plot

# colnames <- dimnames(subset)[[2]]
# for (i in 1:18) {
#   d <- density(unlist(subset[i]))
#   plot(d, type="n", main=colnames[i])
#   polygon(d, col="grey", border="blue")
# }
# summary(subset)

```

```{r, echo=FALSE}
#Plot correlation matrix
library(corrplot)
correlation = cor(subset)
par(mfrow=c(1,1))
corrplot(correlation, method="circle")
#We can see quite strong correlations between many varaibles
# sin_day & sin_month
# cos_day & cos_month
# cos_day & DEWP
# cos_day & TEMP  etc.
```

```{r, echo=FALSE}
# colnames <- dimnames(subset)[[2]]
# for (i in c(1:ncol(dataset)))
# { 
#   boxplot(dataset[,i])
# }
# Seperation subset into X and Y training and test sets  80% / 20%
Y = subset[,'pm2.5']
# scale(X) Normalization input
X = subset[,-2]

#library(matrixStats)
#sdX = colSds(as.matrix(X)) ;meanX = colMeans(X); X = scale(X)
#(train_X - mean(train_X)) / sd(train_X)
#(test_X - mean(train_X)) / sd(train_X)
# Seperation to training and test sets
training_size = 0.8*nrow(subset)
test_size = 0.2*nrow(subset)
train_Y = Y[1:training_size]
train_X = X[1:training_size,]
test_Y = Y[(training_size+1):nrow(subset)]
test_X = X[(training_size+1):nrow(subset),]
```
## Construction Multilinear Models
Model,  $Y=X\beta+\varepsilon$ where $X$ is $n\times (p+k)$ matrix where :
\begin{itemize}
\item $Y$ is the response variable 
\item $X$ is the predictor
\item $n$ is number of observations
\item $p$ is number of variables
\item $k$ dummy variables
\end{itemize}
The vector $\beta=\{\beta_0,\beta_1,\beta_2,...,\beta_k\}$ and $E(\varepsilon)=0, V(\varepsilon)=\sigma^2$ where :
\begin{itemize}
\item $\beta$ is parameter
\item $\varepsilon$ is error term
\end{itemize}
### Linear models
```{r, echo=FALSE}
#Linear regression
reg = lm('train_Y~.', data = as.data.frame(cbind(train_X,train_Y)))
summary(reg)
sum((train_Y-reg$fitted.values)^2)/training_size
```
This linear regression model gets a high R-squared. It should be a good model and it performs quite well on test set because we added variable of pm2.5 at previous time.

### Model selection contructed by stepwise (Forward/Backward)
This algorithm considers all models starting from null model (using only intercept) to more complex model with more variables called Forward regression. Another regression starts from full model (using all variables) to more simple model with less variables called Backward regression. In this case, we will use stepwise regression with both directions and we measure each model using two criteria AIC and BIC. We name those two models with best value of AIC and BIC, step.AIC and step.BIC respectively.
```{r, echo=FALSE}
#### model selection
regbackward = step(reg,direction = 'backward'); 
summary(regbackward)
regforward = step(reg,list(upper=reg),direction = 'forward'); 
summary(regforward)
```
```{r, echo=FALSE}
extractAIC(regbackward,k=log(n))
extractAIC(regforward,k=log(n))

formula(regbackward)
sum((train_Y-regbackward$fitted.values)^2)/training_size
formula(regforward)
sum((train_Y-regforward$fitted.values)^2)/training_size

pred_test_Y = predict(regforward,as.data.frame(test_X))
sum((test_Y-pred_test_Y)^2)/test_size
```

```{r, echo=FALSE}
plot(train_Y, regbackward$residuals)
abline(a = 0,b =0, col="red")
plot(train_Y, regbackward$fitted.values)
abline(a = 0,b =1, col="red")
sum((train_Y-regbackward$fitted.values)^2)/training_size
```
We check for the diagnostic of these two models. These models get a high R-square and their mean square errors are a bit larger than normal linear regression model. The residuals seem to be symmetric in the graphic above. It appears to be good models.

## Penalisation Regression
In this part we are interested in the regularised methods ridge and LASSO in order to constrain the variance of our estimator and eventualy improve our prediction error. We trade between the Residual Sum of Square (RSS) and Variance of the estimator by constraining our estimator in a limited region.
$$\hat{\beta}=arg \displaystyle\min_\beta\|Y-X\beta\|_{2}^2+\lambda\Omega(\beta)$$
### Ridge Regression
The Ridge regression constrains the variance of estimator using $\|.\|_2$. We measure each model through cross-validation with the \textbf{minimum} of GCV (the most penalized model with a "1se" distance from the model with the least error).
$$\hat{\beta}^{ridge}=arg \displaystyle\min_\beta\|Y-X\beta\|_{2}^2+\lambda|\beta\|_2^2$$
We can represent the regularisation path (each path correspond to each predictor) in function of different mesurements (lambda) as shown in graphic below.
```{r, echo=FALSE}
library(MASS)
reg_l2 = lm.ridge('pm2.5~.', data = as.data.frame(subset), lambda = seq(10,15,0.01))
#coef(reg_l2)
par(mfrow=c(1,2))
plot(reg_l2$GCV,pch=16,type='b',col='blue')
plot(reg_l2)
summary(reg_l2)
cf = sort(coef(reg_l2),decreasing = TRUE)
cf[1:5]
attributes(reg_l2)
# select function to select best cross validation
select(reg_l2)
GCV.MIN<-reg_l2$GCV[which.min(reg_l2$GCV)]
reg_l2 = lm.ridge('pm2.5~.', data = as.data.frame(subset), lambda = as.numeric(names(GCV.MIN)))
#plot(reg_l2)
summary(reg_l2)
coef(reg_l2)
predicted =  as.matrix(cbind(const=1,train_X))%*%as.vector(coef(reg_l2))
sum((train_Y-predicted)^2)/training_size
```

### Lasso Regression
It is similar to the Ridge regression. The difference is in the LASSO regression we constrain the variance of estimator using $\|.\|_1$. Because of the nature of this norm, it puts some variables to be 0 and the remaining variables are considered as significant for our model.
$$\hat{\beta}^{lasso}=arg \displaystyle\min_\beta\|Y-X\beta\|_{2}^2+\lambda|\beta\|_1$$

```{r, echo=FALSE}
library(lars)
reg_l1 = lars(as.matrix(train_X),as.matrix(train_Y),type="lasso")
par(mfrow=c(2,1))
plot(reg_l1)
plot(c(reg_l1$lambda,0),pch=16,type='b',col='blue'); grid()

print(coef(reg_l1))
#When the mode is lambda then the s is the lambda value
coef = predict.lars(reg_l1, train_X, type='coefficients',mode='lambda',s=0.4)
coefl1 = coef$coefficients
#par(mforw=c(1,1));
barplot(coefl1, main = 'lasso, l=1', col = 'blue')


coef.2 = predict.lars(reg_l1, train_X, type='coefficients',mode='lambda',s=1000)
#print(coef.2$coefficients)
#print(coef.4)
#print(coef.6)
count = 0;
for (i in c(1:18))
{
  if (!is.na(coef.2$coefficients[i]))
  {
    if (coef.2$coefficients[i]!=0)
    {
      count = count+1;
      print(coef.2$coefficients[i]);
    }
  }
}
print(count);
length(coef.2$coefficients)


## For l1 regularization
fitted = predict.lars(reg_l1, train_X, type='fit',mode='lambda',s=1)
# Mean Square Error
sum((train_Y-fitted$fit)^2)/training_size
```
### Residual of Ridge & Lasso regression

## Elastic net regularization
Elastic net is a hybrid approach that blends both penalization of the L2 and L1 norms. Ridge and LASSO are particular cases of Elastic net.

$$\hat{\beta}^{elas}=arg \displaystyle\min_\beta\|Y-X\beta\|_{2}^2+\lambda((1-\alpha)\|\beta\|_2^2+\alpha\|\beta\|_1)$$
\begin{itemize}
\item If $\alpha = 0 \Leftrightarrow$ ridge regression.
\item If $\alpha = 1 \Leftrightarrow$ LASSO regression.
\item If $0<\alpha<1 \Leftrightarrow$ Elastic net.
\end{itemize}

We consider the following cases with different values of $\alpha\in\{0.05,0.1,...,0.95\}$. The following table is a particular case of Elastic net we obtained.

```{r, echo=FALSE}
library(glmnet)
elastic_net<-function(a){
  cvm<-c()
  lambda.1se<-c()
  alpha<-c()
  for(i in a){
    cv <- cv.glmnet(as.matrix(train_X), as.vector(train_Y),alpha = i)
    cvm = c(cvm,cv$cvm[cv$lambda == cv$lambda.1se])
    lambda.1se = c(lambda.1se,cv$lambda.1se)
    alpha = c(alpha,i)
  }
  tab<-data.frame(cbind(cvm,lambda.1se,alpha))
  return(tab)
}
a <- seq(0.05, 0.95, 0.05)
elastic_tab<-elastic_net(a)
elastic_tab
cv3 <- elastic_tab[elastic_tab$cvm == min(elastic_tab$cvm), ]
cv3
md3 <- glmnet(as.matrix(train_X), as.vector(train_Y), lambda = cv3$lambda.1se, alpha = cv3$alpha)
coef(md3)
predict_elas <- as.matrix(cbind(const=1,train_X))%*%as.vector(coef(md3))
sum((train_Y-predict_elas)^2)/training_size
```
The mean square error of this regularization gets a bit larger than ridge and lasso. Some coefficients are equal to 0(it means we can use less variables). It seems to be good model.



